import json
import time
import pandas as pd
from tqdm import tqdm
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import os
import sys
import django

# --- Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN (PATH) ---
current_dir = os.path.dirname(os.path.abspath(__file__))

# 2. ThÃªm thÆ° má»¥c nÃ y vÃ o sys.path Ä‘á»ƒ Python nhÃ¬n tháº¥y folder 'chattour' bÃªn trong
sys.path.append(current_dir)

# 3. Setup Django (Äá»ƒ cháº¡y Ä‘Æ°á»£c cÃ¡c lá»‡nh liÃªn quan Ä‘áº¿n Database)
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "chatbot.settings")
django.setup()

# --- IMPORT SERVICE (Sá»¬A Äá»”I QUAN TRá»ŒNG) ---
print("ğŸ”Œ Äang káº¿t ná»‘i tá»›i RAG Service...")
try:
    # ÄÆ°á»ng dáº«n báº¡n cung cáº¥p: Rag_Travel_Tour\chatbot\chattour\services.py
    # VÃ¬ ta Ä‘ang Ä‘á»©ng á»Ÿ 'chatbot', nÃªn gá»i vÃ o 'chattour.services'
    from chattour.services import rag_service
    print("âœ… ÄÃ£ import thÃ nh cÃ´ng: rag_service")
except ImportError as e:
    print(f"âŒ Lá»—i Import nghiÃªm trá»ng: {e}")
    print("ğŸ’¡ Gá»£i Ã½ kiá»ƒm tra:")
    print("   1. File 'Rag_Travel_Tour/chatbot/chattour/services.py' cÃ³ tá»“n táº¡i khÃ´ng?")
    print("   2. Trong file Ä‘Ã³, dÃ²ng cuá»‘i cÃ¹ng cÃ³ lá»‡nh 'rag_service = RAGService()' khÃ´ng?")
    sys.exit(1)

# --- PHáº¦N DÆ¯á»šI GIá»® NGUYÃŠN ---
# (Tá»« Ä‘oáº¡n táº£i NLTK vÃ  hÃ m calculate_metrics trá»Ÿ Ä‘i khÃ´ng cáº§n sá»­a)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("â³ Äang táº£i dá»¯ liá»‡u NLTK...")
    nltk.download('punkt')
    nltk.download('punkt_tab')

# ... (Copy tiáº¿p pháº§n cÃ²n láº¡i cá»§a code cÅ© vÃ o Ä‘Ã¢y)
print("â³ Äang táº£i model Ä‘Ã¡nh giÃ¡ (SentenceTransformer)...")
similarity_model = SentenceTransformer('keepitreal/vietnamese-sbert') 
rouge_evaluator = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
chencherry = SmoothingFunction() 

def calculate_metrics(prediction, reference):
    # a. BLEU Score
    ref_tokens = [reference.split()]
    pred_tokens = prediction.split()
    bleu = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=chencherry.method1)
    
    # b. ROUGE-L
    rouge_score = rouge_evaluator.score(reference, prediction)['rougeL'].fmeasure
    
    # c. Semantic Similarity (Cosine)
    embeddings = similarity_model.encode([prediction, reference])
    similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
    
    return bleu, rouge_score, similarity

def run_evaluation(dataset_path='test_dataset.json'):
    base_path = os.path.dirname(os.path.abspath(__file__))
    full_path = os.path.join(base_path, dataset_path)
    
    print(f"ğŸ“‚ Äang Ä‘á»c dá»¯ liá»‡u tá»«: {full_path}")
    
    try:
        with open(full_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
    except FileNotFoundError:
        print("âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file test_dataset.json")
        return

    results = []
    print(f"ğŸš€ Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ trÃªn {len(test_data)} máº«u...")
    
    for item in tqdm(test_data, desc="Processing"):
        start_time = time.time()
        try:
            response = rag_service.chat(item['question'], session_id=f"eval_auto_{item['id']}")
            chatbot_answer = response['answer']
        except Exception as e:
            print(f"\nâŒ Lá»—i máº«u ID {item['id']}: {e}")
            chatbot_answer = "Error"
            
        latency = time.time() - start_time
        bleu, rouge, similarity = calculate_metrics(chatbot_answer, item['ground_truth'])
        
        results.append({
            "id": item['id'],
            "type": item.get('type', 'general'),
            "question": item['question'],
            "ground_truth": item['ground_truth'],
            "chatbot_answer": chatbot_answer,
            "bleu": round(bleu, 4),
            "rouge_l": round(rouge, 4),
            "similarity": round(similarity, 4),
            "latency": round(latency, 2)
        })
        time.sleep(3) 

    output_file = 'evaluation_results.csv'
    df = pd.DataFrame(results)
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    
    print(f"\nâœ… ÄÃ£ xuáº¥t káº¿t quáº£ ra file: {output_file}")
    print("-" * 50)
    print("ğŸ“Š Káº¾T QUáº¢ TRUNG BÃŒNH:")
    print(f"   â¤ BLEU Score: {df['bleu'].mean():.4f}")
    print(f"   â¤ ROUGE-L:    {df['rouge_l'].mean():.4f}")
    print(f"   â¤ Similarity: {df['similarity'].mean():.4f}")
    print(f"   â¤ Latency:    {df['latency'].mean():.2f}s")
    print("-" * 50)

if __name__ == "__main__":
    run_evaluation()